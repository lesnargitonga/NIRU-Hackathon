## Project Description
Lesnar AI (Operation Sentinel) is a purely vision-based autonomous drone system designed for GPS-denied perimeter defense.

Unlike traditional drones that rely on GPS or pre-mapped waypoints, Sentinel uses a "Universal Cortex" trained in procedural simulations (Gazebo Harmonic). It navigates using raw sensor data (LiDAR/Camera), making it immune to GPS jamming.

## Inspiration
Current military/security drones are expensive and fragile. If you jam their GPS, they drift or crash. We wanted to build a "Biological" drone that sees the world like a birdâ€”reacting to obstacles in real-time using an onboard neural network, not a map.

## How we built it
1. **Simulation:** We switched from AirSim to **Gazebo Harmonic + PX4** to create a lightweight, high-fidelity physics environment accessible via WSL2.
2. **Teacher-Student Learning:** We built a "God Mode" teacher (A* pathfinding with access to ground-truth maps) to generate perfect flight trajectories.
3. **The Brain:** We trained a PPO (Proximal Policy Optimization) agent to mimic this teacher using only local sensor inputs (LiDAR scans + Velocity vectors).
4. **Integration:** The system runs on a Windows/WSL hybrid architecture, bridging the simulated world (WSL) with the control logic (Windows/Conda) via high-speed MAVLink/gRPC.

## Challenges we ran into
*   **The Sim Gap:** AirSim was heavy and deprecated. Migrating to PX4/Gazebo saved the project but required rebuilding the entire networking stack (Windows-to-WSL bridge).
*   **Networking Hell:** Crossing the boundary between Windows (Control) and WSL2 (Simulation) required custom firewall rules and MAVSDK bridging.
*   **Stability:** Training a drone that doesn't crash is hard. We implemented a "Reflex Layer" that limits acceleration and yaw rates to keep the AI within safe flight envelopes.
